{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ETL_THCovid19_Sum",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1ypdBASkCsHhRr95qLCxP8Y_YdEyG-4JM",
      "authorship_tag": "ABX9TyNdQlG75TRkA5Q9E7H6Iy0r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThanaVi/THCovid19_Report/blob/master/ETL_THCovid19_Sum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrPOIsA6YCCj",
        "colab_type": "text"
      },
      "source": [
        "# Target\n",
        "\n",
        "1. จำนวนผู้ป่วย Covid19 จากข้อมูลผู้ป่วย Covid19 ที่เตรียมไว้\n",
        "2. นำข้อมูลโหลดเข้า BigQuery"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVJUh6KUocdE",
        "colab_type": "text"
      },
      "source": [
        "#Result\n",
        "\n",
        "ข้อมูลจาก Source\n",
        "![alt text](https://drive.google.com/uc?id=1kIi-suj7SuI0R9yBTCfgHvFxQqT_pSPt)\n",
        "\n",
        "ข้อมูลที่ได้จาก Transformation\n",
        "![alt text](https://drive.google.com/uc?id=1wfI2JRsBVawv8Ho6fMHs8pZlVvdDd26e)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62D5ciS1ZSZI",
        "colab_type": "text"
      },
      "source": [
        "#Install Pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwfQKZmXTwc6",
        "colab_type": "code",
        "outputId": "b908bd86-fefe-44b6-96a4-ea554432af37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xzvf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\"\n",
        "\n",
        "!pip install pyspark"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spark-2.4.5-bin-hadoop2.7/\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-jtransforms.html\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-zstd.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-zstd-jni.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-xmlenc.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-vis.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-spire.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-sorttable.js.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-slf4j.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-scopt.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-scala.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-sbt-launch-lib.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-respond.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-reflectasm.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-pyrolite.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-py4j.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-protobuf.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-pmml-model.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-paranamer.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-netlib.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-mustache.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-modernizr.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-minlog.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-matchMedia-polyfill.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-machinist.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-leveldbjni.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-kryo.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-json-formatter.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-jquery.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-join.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-jodd.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-jline.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-javolution.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-javassist.html\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-janino.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-heapq.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-graphlib-dot.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-f2j.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-datatables.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-dagre-d3.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-d3.min.js.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-cloudpickle.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-bootstrap.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-automaton.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-arpack.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-antlr.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-CC0.txt\n",
            "spark-2.4.5-bin-hadoop2.7/licenses/LICENSE-AnchorJS.txt\n",
            "spark-2.4.5-bin-hadoop2.7/LICENSE\n",
            "spark-2.4.5-bin-hadoop2.7/examples/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/jars/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/examples/jars/scopt_2.11-3.7.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRecommendationExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestRegressionExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPrefixSpanExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPowerIterationClusteringExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaPCAExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaNaiveBayesExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaMulticlassClassificationMetricsExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaMultiLabelClassificationMetricsExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLogisticRegressionWithLBFGSExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLatentDirichletAllocationExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLBFGSExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaKernelDensityEstimationExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaKMeansExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaIsotonicRegressionExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingKolmogorovSmirnovTestExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingRegressionExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingClassificationExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaGaussianMixtureExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaElementwiseProductExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeRegressionExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeClassificationExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaChiSqSelectorExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaBisectingKMeansExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaBinaryClassificationMetricsExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaAssociationRulesExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaALS.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRegressionMetricsExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRankingMetricsExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestClassificationExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaLinearRegressionWithSGDExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaCorrelationsExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSummaryStatisticsExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaStreamingTestExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaStratifiedSamplingExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSimpleFPGrowth.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVMWithSGDExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVDExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestClassifierExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPrefixSpanExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaOneHotEncoderEstimatorExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeClassifierExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeClassificationExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaALSExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaWord2VecExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSlicerExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSizeHintExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorIndexerExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorAssemblerExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaTokenizerExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaTfIdfExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaSummarizerExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStringIndexerExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaStandardScalerExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaSQLTransformerExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestRegressorExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaRFormulaExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaQuantileDiscretizerExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPowerIterationClusteringExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPolynomialExpansionExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaPCAExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaOneVsRestExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNaiveBayesExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaNGramExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMultilayerPerceptronClassifierExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMulticlassLogisticRegressionWithElasticNetExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaTrainValidationSplitExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMinMaxScalerExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMinHashLSHExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaMaxAbsScalerExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionWithElasticNetExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionSummaryExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearSVCExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearRegressionWithElasticNetExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLabeledDocument.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaLDAExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaIsotonicRegressionExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaInteractionExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaIndexToStringExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaImputerExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeRegressorExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGeneralizedLinearRegressionExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaGaussianMixtureExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFeatureHasherExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaFPGrowthExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaEstimatorTransformerParamExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaElementwiseProductExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDocument.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeRegressionExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaDCTExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaCountVectorizerExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaCorrelationExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSquareTestExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSqSelectorExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketizerExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketedRandomProjectionLSHExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaBinarizerExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/ml/JavaAFTSurvivalRegressionExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaWordCount.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaStatusTrackerDemo.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaSparkPi.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaPageRank.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaLogQuery.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaHdfsLR.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/JavaTC.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaStatefulNetworkWordCount.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaSqlNetworkWordCount.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecoverableNetworkWordCount.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecord.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaQueueStream.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaNetworkWordCount.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKafkaWordCount.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/streaming/JavaCustomReceiver.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedUntypedAggregation.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredSessionization.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCountWindowed.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCount.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKafkaWordCount.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/hive/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/hive/JavaSparkHiveExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedTypedAggregation.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkTC.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkRemoteFileTest.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkPageRank.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkLR.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkKMeans.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkHdfsLR.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SparkALS.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SkewedGroupByTest.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/SimpleSkewedGroupByTest.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/MultiBroadcastTest.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LogQuery.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalPi.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalLR.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalKMeans.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalFileLR.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/LocalALS.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/HdfsTest.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/GroupByTest.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ExceptionHandlingTest.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/DriverSubmissionTest.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/DFSReadWriteTest.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/BroadcastTest.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/StreamingExamples.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/StatefulNetworkWordCount.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/SqlNetworkWordCount.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/RawNetworkGrep.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/QueueStream.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/HdfsWordCount.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/CustomReceiver.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewStream.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewGenerator.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedUntypedAggregation.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/RDDRelation.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCountWindowed.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCount.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredSessionization.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKafkaWordCount.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/hive/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/hive/SparkHiveExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedTypedAggregation.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/pythonconverters/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/pythonconverters/AvroConverters.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnySVD.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnyPCA.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingTestExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLogisticRegression.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingKMeansExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SparseNaiveBayes.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SimpleFPGrowth.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SampledRDDs.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RegressionMetricsExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RankingMetricsExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomRDDGeneration.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PrefixSpanExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PowerIterationClusteringExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MultivariateSummarizer.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MovieLensALS.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LinearRegressionWithSGDExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LinearRegression.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LatentDirichletAllocationExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LDAExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/KMeansExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostedTreesRunner.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GaussianMixtureExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/FPGrowthExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/ElementwiseProductExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DenseKMeans.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRunner.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/CosineSimilarity.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/Correlations.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BisectingKMeansExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassification.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/AssociationRulesExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/Word2VecExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/TFIDFExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SummaryStatisticsExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLinearRegressionExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StratifiedSamplingExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/StandardScalerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SVMWithSGDExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/SVDExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RecommendationExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestRegressionExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestClassificationExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PMMLModelExportExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnSourceVectorExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnRowMatrixExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/NormalizerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/NaiveBayesExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MulticlassMetricsExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/MultiLabelMetricsExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LogisticRegressionWithLBFGSExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/LBFGSExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/KernelDensityEstimationExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/IsotonicRegressionExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingKolmogorovSmirnovTestExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingRegressionExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingClassificationExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRegressionExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeClassificationExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/CorrelationsExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/ChiSqSelectorExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassificationMetricsExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/mllib/AbstractParams.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/Word2VecExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/UnaryTransformerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/TfIdfExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/SQLTransformerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/QuantileDiscretizerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PrefixSpanExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/OneVsRestExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/OneHotEncoderEstimatorExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/IndexToStringExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeClassifierExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GBTExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FPGrowthExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DeveloperApiExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeClassificationExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DataFrameExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/CountVectorizerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ChiSqSelectorExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BisectingKMeansExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ALSExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorSlicerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorSizeHintExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorIndexerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/VectorAssemblerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/TokenizerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/SummarizerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StringIndexerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StopWordsRemoverExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/StandardScalerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestRegressorExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/RFormulaExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PowerIterationClusteringExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PolynomialExpansionExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PipelineExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BinarizerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/AFTSurvivalRegressionExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/PCAExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NormalizerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NaiveBayesExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/NGramExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MultilayerPerceptronClassifierExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MulticlassLogisticRegressionWithElasticNetExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaTrainValidationSplitExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaCrossValidationExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MinMaxScalerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MinHashLSHExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/MaxAbsScalerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionWithElasticNetExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionSummaryExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearSVCExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionWithElasticNetExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/LDAExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/KMeansExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/IsotonicRegressionExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/InteractionExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ImputerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeRegressorExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GeneralizedLinearRegressionExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/GaussianMixtureExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/FeatureHasherExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/EstimatorTransformerParamExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ElementwiseProductExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeRegressionExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/DCTExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/CorrelationExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/ChiSquareTestExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BucketizerExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/ml/BucketedRandomProjectionLSHExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/SynthBenchmark.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/LiveJournalPageRank.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/TriangleCountingExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/SSSPExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/PageRankExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/ConnectedComponentsExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/ComprehensiveExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/Analytics.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/scala/org/apache/spark/examples/graphx/AggregateMessagesExample.scala\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/resources/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/resources/users.parquet\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/resources/users.orc\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/resources/users.avro\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/resources/user.avsc\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/resources/people.txt\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/resources/people.json\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/resources/people.csv\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/resources/kv1.txt\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/resources/full_user.avsc\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/resources/employees.json\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/RSparkSQLExample.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/streaming/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/streaming/structured_network_wordcount.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/svmLinear.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/survreg.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/randomForest.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/naiveBayes.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/mlp.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/ml.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/logit.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/lda.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/kstest.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/kmeans.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/isoreg.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/glm.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/gbt.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/gaussianMixture.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/fpm.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/decisionTree.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/bisectingKmeans.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/ml/als.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/dataframe.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/r/data-manipulation.R\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/wordcount.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/transitive_closure.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/streaming/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/streaming/recoverable_network_wordcount.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/streaming/kafka_wordcount.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/streaming/flume_wordcount.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/streaming/direct_kafka_wordcount.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/streaming/stateful_network_wordcount.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/streaming/sql_network_wordcount.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/streaming/queue_stream.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/streaming/network_wordjoinsentiments.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/streaming/network_wordcount.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/streaming/hdfs_wordcount.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/status_api_demo.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/sql/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/sql/hive.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/sql/datasource.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/sql/arrow.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/sql/streaming/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_network_wordcount_windowed.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_network_wordcount.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/sql/streaming/structured_kafka_wordcount.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/sql/basic.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/sort.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/pi.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/parquet_inputformat.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/pagerank.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/standard_scaler_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/ranking_metrics_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/multi_class_metrics_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/isotonic_regression_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/bisecting_k_means_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/word2vec_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/word2vec.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/tf_idf_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/svm_with_sgd_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/svd_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/summary_statistics_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/streaming_linear_regression_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/streaming_k_means_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/stratified_sampling_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/sampled_rdds.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/regression_metrics_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/recommendation_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/random_rdd_generation.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/random_forest_regression_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/random_forest_classification_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/power_iteration_clustering_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/pca_rowmatrix_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/normalizer_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/naive_bayes_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/multi_label_metrics_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/logistic_regression_with_lbfgs_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/logistic_regression.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/linear_regression_with_sgd_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/latent_dirichlet_allocation_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/kmeans.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/kernel_density_estimation_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/k_means_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/hypothesis_testing_kolmogorov_smirnov_test_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/hypothesis_testing_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/gradient_boosting_regression_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/gradient_boosting_classification_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/gaussian_mixture_model.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/gaussian_mixture_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/fpgrowth_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/elementwise_product_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/decision_tree_regression_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/decision_tree_classification_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/correlations_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/correlations.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/mllib/binary_classification_metrics_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/onehot_encoder_estimator_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/bisecting_k_means_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/word2vec_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/vector_slicer_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/vector_size_hint_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/vector_indexer_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/vector_assembler_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/train_validation_split.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/tokenizer_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/tf_idf_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/summarizer_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/string_indexer_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/stopwords_remover_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/standard_scaler_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/sql_transformer.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/rformula_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/random_forest_regressor_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/random_forest_classifier_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/quantile_discretizer_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/prefixspan_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/polynomial_expansion_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/pipeline_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/pca_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/one_vs_rest_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/normalizer_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/naive_bayes_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/n_gram_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/multilayer_perceptron_classification.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/multiclass_logistic_regression_with_elastic_net.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/min_max_scaler_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/min_hash_lsh_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/max_abs_scaler_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/logistic_regression_with_elastic_net.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/logistic_regression_summary_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/linearsvc.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/linear_regression_with_elastic_net.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/lda_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/kmeans_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/isotonic_regression_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/index_to_string_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/imputer_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/gradient_boosted_tree_regressor_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/gradient_boosted_tree_classifier_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/generalized_linear_regression_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/gaussian_mixture_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/fpgrowth_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/feature_hasher_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/estimator_transformer_param_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/elementwise_product_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/decision_tree_regression_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/decision_tree_classification_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/dct_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/dataframe_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/cross_validator.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/count_vectorizer_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/correlation_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/chisq_selector_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/chi_square_test_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/bucketizer_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/bucketed_random_projection_lsh_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/binarizer_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/als_example.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/ml/aft_survival_regression.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/logistic_regression.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/kmeans.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/avro_inputformat.py\n",
            "spark-2.4.5-bin-hadoop2.7/examples/src/main/python/als.py\n",
            "spark-2.4.5-bin-hadoop2.7/kubernetes/\n",
            "spark-2.4.5-bin-hadoop2.7/kubernetes/dockerfiles/\n",
            "spark-2.4.5-bin-hadoop2.7/kubernetes/dockerfiles/spark/\n",
            "spark-2.4.5-bin-hadoop2.7/kubernetes/dockerfiles/spark/entrypoint.sh\n",
            "spark-2.4.5-bin-hadoop2.7/kubernetes/dockerfiles/spark/Dockerfile\n",
            "spark-2.4.5-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/\n",
            "spark-2.4.5-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/python/\n",
            "spark-2.4.5-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/python/Dockerfile\n",
            "spark-2.4.5-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/R/\n",
            "spark-2.4.5-bin-hadoop2.7/kubernetes/dockerfiles/spark/bindings/R/Dockerfile\n",
            "spark-2.4.5-bin-hadoop2.7/kubernetes/tests/\n",
            "spark-2.4.5-bin-hadoop2.7/kubernetes/tests/pyfiles.py\n",
            "spark-2.4.5-bin-hadoop2.7/kubernetes/tests/worker_memory_check.py\n",
            "spark-2.4.5-bin-hadoop2.7/kubernetes/tests/py_container_checks.py\n",
            "spark-2.4.5-bin-hadoop2.7/yarn/\n",
            "spark-2.4.5-bin-hadoop2.7/yarn/spark-2.4.5-yarn-shuffle.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/\n",
            "spark-2.4.5-bin-hadoop2.7/jars/scala-compiler-2.11.12.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/pyrolite-4.13.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/py4j-0.10.7.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/protobuf-java-2.5.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/parquet-jackson-1.10.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/parquet-hadoop-bundle-1.6.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/parquet-hadoop-1.10.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/parquet-format-2.4.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/parquet-encoding-1.10.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/parquet-common-1.10.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/parquet-column-1.10.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/paranamer-2.8.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/osgi-resource-locator-1.0.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/oro-2.0.8.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/orc-shims-1.5.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/orc-mapreduce-1.5.5-nohive.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/orc-core-1.5.5-nohive.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/opencsv-2.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/okio-1.15.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/okhttp-3.12.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/objenesis-2.5.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/netty-all-4.1.42.Final.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/netty-3.9.9.Final.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/minlog-1.3.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/metrics-jvm-3.1.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/metrics-json-3.1.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/metrics-graphite-3.1.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/metrics-core-3.1.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/mesos-1.4.0-shaded-protobuf.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/macro-compat_2.11-1.1.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/machinist_2.11-0.6.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/lz4-java-1.4.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/logging-interceptor-3.12.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/zstd-jni-1.3.2-2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/zookeeper-3.4.6.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/zjsonpatch-0.3.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/xz-1.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/xmlenc-0.52.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/xercesImpl-2.9.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/xbean-asm6-shaded-4.8.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/validation-api-1.1.0.Final.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/univocity-parsers-2.7.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/super-csv-2.2.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/stringtemplate-3.2.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/stream-2.7.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/stax-api-1.0.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/stax-api-1.0-2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spire_2.11-0.13.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spire-macros_2.11-0.13.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-yarn_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-unsafe_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-tags_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-tags_2.11-2.4.5-tests.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-streaming_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-sql_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-sketch_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-repl_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-network-shuffle_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-network-common_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-mllib_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-mllib-local_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-mesos_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-launcher_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-kvstore_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-kubernetes_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-hive_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-hive-thriftserver_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-graphx_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-core_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/spark-catalyst_2.11-2.4.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/snappy-java-1.1.7.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/snappy-0.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/snakeyaml-1.15.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/slf4j-log4j12-1.7.16.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/slf4j-api-1.7.16.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/shims-0.7.45.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/shapeless_2.11-2.3.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/scala-xml_2.11-1.0.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/scala-reflect-2.11.12.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/scala-parser-combinators_2.11-1.1.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/scala-library-2.11.12.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/log4j-1.2.17.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/libthrift-0.9.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/libfb303-0.9.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/leveldbjni-all-1.8.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/kubernetes-model-common-4.6.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/kubernetes-model-4.6.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/kubernetes-client-4.6.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/kryo-shaded-4.0.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jul-to-slf4j-1.7.16.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jtransforms-2.4.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jta-1.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jsr305-1.3.9.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jsp-api-2.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/json4s-scalap_2.11-3.5.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/json4s-jackson_2.11-3.5.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/json4s-core_2.11-3.5.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/json4s-ast_2.11-3.5.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jpam-1.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jodd-core-3.5.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/joda-time-2.9.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jline-2.14.6.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jetty-util-6.1.26.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jetty-6.1.26.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jersey-server-2.22.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jersey-media-jaxb-2.22.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jersey-guava-2.22.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jersey-container-servlet-core-2.22.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jersey-container-servlet-2.22.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jersey-common-2.22.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jersey-client-2.22.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jdo-api-3.0.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jcl-over-slf4j-1.7.16.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jaxb-api-2.2.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/javolution-5.5.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/javax.ws.rs-api-2.0.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/javax.servlet-api-3.1.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/javax.inject-2.4.0-b34.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/javax.inject-1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/javax.annotation-api-1.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/javassist-3.18.1-GA.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/janino-3.0.9.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jackson-xc-1.9.13.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jackson-module-scala_2.11-2.6.7.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jackson-module-paranamer-2.7.9.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jackson-module-jaxb-annotations-2.6.7.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jackson-mapper-asl-1.9.13.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jackson-jaxrs-1.9.13.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jackson-dataformat-yaml-2.6.7.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jackson-databind-2.6.7.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jackson-core-asl-1.9.13.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jackson-core-2.6.7.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/jackson-annotations-2.6.7.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/ivy-2.4.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/httpcore-4.4.10.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/httpclient-4.5.6.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/htrace-core-3.1.0-incubating.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hppc-0.7.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hk2-utils-2.4.0-b34.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hk2-locator-2.4.0-b34.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hk2-api-2.4.0-b34.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hive-metastore-1.2.1.spark2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hive-jdbc-1.2.1.spark2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hive-exec-1.2.1.spark2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hive-cli-1.2.1.spark2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hive-beeline-1.2.1.spark2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hadoop-yarn-server-web-proxy-2.7.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hadoop-yarn-server-common-2.7.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hadoop-yarn-common-2.7.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hadoop-yarn-client-2.7.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hadoop-yarn-api-2.7.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hadoop-mapreduce-client-shuffle-2.7.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hadoop-mapreduce-client-jobclient-2.7.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hadoop-mapreduce-client-core-2.7.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hadoop-mapreduce-client-common-2.7.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hadoop-mapreduce-client-app-2.7.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hadoop-hdfs-2.7.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hadoop-common-2.7.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hadoop-client-2.7.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hadoop-auth-2.7.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/hadoop-annotations-2.7.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/guice-servlet-3.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/guice-3.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/guava-14.0.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/gson-2.2.4.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/generex-1.0.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/flatbuffers-1.2.0-3f79e055.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/eigenbase-properties-1.1.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/derby-10.12.1.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/datanucleus-rdbms-3.2.9.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/datanucleus-core-3.2.10.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/datanucleus-api-jdo-3.2.6.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/curator-recipes-2.7.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/curator-framework-2.7.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/curator-client-2.7.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/core-1.1.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/compress-lzf-1.0.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/commons-pool-1.5.4.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/commons-net-3.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/commons-math3-3.4.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/commons-logging-1.1.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/commons-lang3-3.5.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/commons-lang-2.6.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/commons-io-2.4.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/commons-httpclient-3.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/commons-digester-1.8.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/commons-dbcp-1.4.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/commons-crypto-1.0.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/commons-configuration-1.6.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/commons-compress-1.8.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/commons-compiler-3.0.9.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/commons-collections-3.2.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/commons-codec-1.10.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/commons-cli-1.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/commons-beanutils-1.9.4.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/chill_2.11-0.9.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/chill-java-0.9.3.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/calcite-linq4j-1.2.0-incubating.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/calcite-core-1.2.0-incubating.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/calcite-avatica-1.2.0-incubating.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/breeze_2.11-0.13.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/breeze-macros_2.11-0.13.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/bonecp-0.8.0.RELEASE.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/avro-mapred-1.8.2-hadoop2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/avro-ipc-1.8.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/avro-1.8.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/automaton-1.11-8.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/arrow-vector-0.10.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/arrow-memory-0.10.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/arrow-format-0.10.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/arpack_combined_all-0.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/api-util-1.0.0-M20.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/api-asn1-api-1.0.0-M20.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/apacheds-kerberos-codec-2.0.0-M15.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/apacheds-i18n-2.0.0-M15.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/apache-log4j-extras-1.2.17.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/aopalliance-repackaged-2.4.0-b34.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/aopalliance-1.0.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/antlr4-runtime-4.7.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/antlr-runtime-3.4.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/antlr-2.7.7.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/aircompressor-0.10.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/activation-1.1.1.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/ST4-4.0.4.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/RoaringBitmap-0.7.45.jar\n",
            "spark-2.4.5-bin-hadoop2.7/jars/JavaEWAH-0.3.2.jar\n",
            "spark-2.4.5-bin-hadoop2.7/RELEASE\n",
            "spark-2.4.5-bin-hadoop2.7/R/\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/sparkr.zip\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/INDEX\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/html/\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/html/R.css\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/html/00Index.html\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/help/\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/help/aliases.rds\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/help/AnIndex\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/help/SparkR.rdx\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/help/SparkR.rdb\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/help/paths.rds\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/worker/\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/worker/worker.R\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/worker/daemon.R\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/tests/\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/tests/testthat/\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/tests/testthat/test_basic.R\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/profile/\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/profile/shell.R\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/profile/general.R\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/R/\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/R/SparkR.rdx\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/R/SparkR.rdb\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/R/SparkR\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/Meta/\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/Meta/nsInfo.rds\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/Meta/links.rds\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/Meta/hsearch.rds\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/Meta/Rd.rds\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/Meta/features.rds\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/Meta/package.rds\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/DESCRIPTION\n",
            "spark-2.4.5-bin-hadoop2.7/R/lib/SparkR/NAMESPACE\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/stop-shuffle-service.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/start-thriftserver.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/start-slave.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/start-shuffle-service.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/start-mesos-shuffle-service.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/start-master.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/start-history-server.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/spark-config.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/stop-thriftserver.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/stop-slaves.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/stop-slave.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/stop-mesos-shuffle-service.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/stop-mesos-dispatcher.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/stop-master.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/stop-history-server.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/stop-all.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/start-slaves.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/start-mesos-dispatcher.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/start-all.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/spark-daemons.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/spark-daemon.sh\n",
            "spark-2.4.5-bin-hadoop2.7/sbin/slaves.sh\n",
            "spark-2.4.5-bin-hadoop2.7/python/\n",
            "spark-2.4.5-bin-hadoop2.7/python/dist/\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark.egg-info/\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark.egg-info/SOURCES.txt\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark.egg-info/dependency_links.txt\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark.egg-info/top_level.txt\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark.egg-info/PKG-INFO\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark.egg-info/requires.txt\n",
            "spark-2.4.5-bin-hadoop2.7/python/setup.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/run-tests.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/run-tests-with-coverage\n",
            "spark-2.4.5-bin-hadoop2.7/python/run-tests\n",
            "spark-2.4.5-bin-hadoop2.7/python/README.md\n",
            "spark-2.4.5-bin-hadoop2.7/python/MANIFEST.in\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/userlibrary.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/userlib-0.1.zip\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/text-test.txt\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/streaming/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/streaming/text-test.txt\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/people_array_utf16le.json\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/people_array.json\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/people1.json\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/people.json\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/part-r-00007.gz.parquet\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=9/day=1/.part-r-00007.gz.parquet.crc\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/part-r-00005.gz.parquet\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=26/.part-r-00005.gz.parquet.crc\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00004.gz.parquet\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/part-r-00002.gz.parquet\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00004.gz.parquet.crc\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2015/month=10/day=25/.part-r-00002.gz.parquet.crc\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/part-r-00008.gz.parquet\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/year=2014/month=9/day=1/.part-r-00008.gz.parquet.crc\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_metadata\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_common_metadata\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/parquet_partitioned/_SUCCESS\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/orc_partitioned/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=1/c=1/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/orc_partitioned/b=0/c=0/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/orc_partitioned/_SUCCESS\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/ages_newlines.csv\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/sql/ages.csv\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/hello/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/hello/sub_hello/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/hello/sub_hello/sub_hello.txt\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/hello/hello.txt\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_support/SimpleHTTPServer.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_coverage/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_coverage/sitecustomize.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_coverage/coverage_daemon.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_coverage/conf/\n",
            "spark-2.4.5-bin-hadoop2.7/python/test_coverage/conf/spark-defaults.conf\n",
            "spark-2.4.5-bin-hadoop2.7/python/setup.cfg\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/python/\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/python/pyspark/\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/python/pyspark/shell.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/worker.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/version.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/util.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/tests.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/test_serializers.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/test_broadcast.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/taskcontext.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/storagelevel.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/shuffle.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/serializers.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/resultiterable.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/rdd.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/profiler.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/java_gateway.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/files.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/daemon.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/context.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/conf.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/cloudpickle.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/broadcast.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/accumulators.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/__init__.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/traceback_utils.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/streaming/\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/streaming/tests.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/streaming/kinesis.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/streaming/kafka.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/streaming/flume.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/streaming/dstream.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/streaming/context.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/streaming/util.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/streaming/listener.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/streaming/__init__.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/status.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/statcounter.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/window.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/utils.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/udf.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/types.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/tests.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/streaming.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/session.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/readwriter.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/group.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/functions.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/dataframe.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/context.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/column.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/catalog.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/__init__.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/sql/conf.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/shell.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/rddsampler.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/util.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/tree.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/tests.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/regression.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/recommendation.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/random.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/fpm.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/feature.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/evaluation.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/clustering.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/classification.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/stat/\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/stat/_statistics.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/stat/test.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/stat/distribution.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/stat/__init__.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/stat/KernelDensity.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/linalg/\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/linalg/distributed.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/linalg/__init__.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/common.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/mllib/__init__.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/wrapper.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/util.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/tuning.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/tests.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/stat.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/regression.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/recommendation.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/image.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/fpm.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/feature.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/evaluation.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/clustering.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/classification.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/base.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/pipeline.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/param/\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/param/shared.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/param/_shared_params_code_gen.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/param/__init__.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/linalg/\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/linalg/__init__.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/common.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/ml/__init__.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/join.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/heapq3.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/find_spark_home.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pyspark/_globals.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/pylintrc\n",
            "spark-2.4.5-bin-hadoop2.7/python/lib/\n",
            "spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip\n",
            "spark-2.4.5-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip\n",
            "spark-2.4.5-bin-hadoop2.7/python/lib/PY4J_LICENSE.txt\n",
            "spark-2.4.5-bin-hadoop2.7/python/docs/\n",
            "spark-2.4.5-bin-hadoop2.7/python/docs/pyspark.streaming.rst\n",
            "spark-2.4.5-bin-hadoop2.7/python/docs/pyspark.sql.rst\n",
            "spark-2.4.5-bin-hadoop2.7/python/docs/pyspark.ml.rst\n",
            "spark-2.4.5-bin-hadoop2.7/python/docs/epytext.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/docs/conf.py\n",
            "spark-2.4.5-bin-hadoop2.7/python/docs/Makefile\n",
            "spark-2.4.5-bin-hadoop2.7/python/docs/pyspark.rst\n",
            "spark-2.4.5-bin-hadoop2.7/python/docs/pyspark.mllib.rst\n",
            "spark-2.4.5-bin-hadoop2.7/python/docs/make2.bat\n",
            "spark-2.4.5-bin-hadoop2.7/python/docs/make.bat\n",
            "spark-2.4.5-bin-hadoop2.7/python/docs/index.rst\n",
            "spark-2.4.5-bin-hadoop2.7/python/docs/_templates/\n",
            "spark-2.4.5-bin-hadoop2.7/python/docs/_templates/layout.html\n",
            "spark-2.4.5-bin-hadoop2.7/python/docs/_static/\n",
            "spark-2.4.5-bin-hadoop2.7/python/docs/_static/pyspark.js\n",
            "spark-2.4.5-bin-hadoop2.7/python/docs/_static/pyspark.css\n",
            "spark-2.4.5-bin-hadoop2.7/python/.gitignore\n",
            "spark-2.4.5-bin-hadoop2.7/python/.coveragerc\n",
            "spark-2.4.5-bin-hadoop2.7/bin/\n",
            "spark-2.4.5-bin-hadoop2.7/bin/spark-class\n",
            "spark-2.4.5-bin-hadoop2.7/bin/pyspark2.cmd\n",
            "spark-2.4.5-bin-hadoop2.7/bin/pyspark\n",
            "spark-2.4.5-bin-hadoop2.7/bin/load-spark-env.sh\n",
            "spark-2.4.5-bin-hadoop2.7/bin/load-spark-env.cmd\n",
            "spark-2.4.5-bin-hadoop2.7/bin/docker-image-tool.sh\n",
            "spark-2.4.5-bin-hadoop2.7/bin/sparkR2.cmd\n",
            "spark-2.4.5-bin-hadoop2.7/bin/sparkR.cmd\n",
            "spark-2.4.5-bin-hadoop2.7/bin/sparkR\n",
            "spark-2.4.5-bin-hadoop2.7/bin/spark-submit2.cmd\n",
            "spark-2.4.5-bin-hadoop2.7/bin/spark-submit.cmd\n",
            "spark-2.4.5-bin-hadoop2.7/bin/spark-submit\n",
            "spark-2.4.5-bin-hadoop2.7/bin/spark-sql2.cmd\n",
            "spark-2.4.5-bin-hadoop2.7/bin/spark-sql.cmd\n",
            "spark-2.4.5-bin-hadoop2.7/bin/spark-sql\n",
            "spark-2.4.5-bin-hadoop2.7/bin/spark-shell2.cmd\n",
            "spark-2.4.5-bin-hadoop2.7/bin/spark-shell.cmd\n",
            "spark-2.4.5-bin-hadoop2.7/bin/spark-shell\n",
            "spark-2.4.5-bin-hadoop2.7/bin/spark-class2.cmd\n",
            "spark-2.4.5-bin-hadoop2.7/bin/spark-class.cmd\n",
            "spark-2.4.5-bin-hadoop2.7/bin/run-example.cmd\n",
            "spark-2.4.5-bin-hadoop2.7/bin/run-example\n",
            "spark-2.4.5-bin-hadoop2.7/bin/pyspark.cmd\n",
            "spark-2.4.5-bin-hadoop2.7/bin/find-spark-home.cmd\n",
            "spark-2.4.5-bin-hadoop2.7/bin/find-spark-home\n",
            "spark-2.4.5-bin-hadoop2.7/bin/beeline.cmd\n",
            "spark-2.4.5-bin-hadoop2.7/bin/beeline\n",
            "spark-2.4.5-bin-hadoop2.7/README.md\n",
            "spark-2.4.5-bin-hadoop2.7/conf/\n",
            "spark-2.4.5-bin-hadoop2.7/conf/spark-env.sh.template\n",
            "spark-2.4.5-bin-hadoop2.7/conf/spark-defaults.conf.template\n",
            "spark-2.4.5-bin-hadoop2.7/conf/slaves.template\n",
            "spark-2.4.5-bin-hadoop2.7/conf/metrics.properties.template\n",
            "spark-2.4.5-bin-hadoop2.7/conf/log4j.properties.template\n",
            "spark-2.4.5-bin-hadoop2.7/conf/fairscheduler.xml.template\n",
            "spark-2.4.5-bin-hadoop2.7/conf/docker.properties.template\n",
            "spark-2.4.5-bin-hadoop2.7/data/\n",
            "spark-2.4.5-bin-hadoop2.7/data/streaming/\n",
            "spark-2.4.5-bin-hadoop2.7/data/streaming/AFINN-111.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/streaming_kmeans_data_test.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/sample_svm_data.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/sample_multiclass_classification_data.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/sample_movielens_data.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/sample_linear_regression_data.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/sample_libsvm_data.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/sample_lda_libsvm_data.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/sample_lda_data.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/sample_kmeans_data.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/sample_isotonic_regression_libsvm_data.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/sample_fpgrowth.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/sample_binary_classification_data.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/ridge-data/\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/ridge-data/lpsa.data\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/pic_data.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/pagerank_data.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/kmeans_data.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/iris_libsvm.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/partitioned/\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-02/\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-02/grayscale.jpg\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-02/chr30.4.184.jpg\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-01/\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-01/BGRA_alpha_60.png\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/partitioned/cls=multichannel/date=2018-01/BGRA.png\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/DP802813.jpg\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/DP153539.jpg\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-02/54893.jpg\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-01/\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-01/not-image.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/partitioned/cls=kittens/date=2018-01/29.5.a_b_EGDP022204.jpg\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/origin/\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/origin/multi-channel/\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/origin/multi-channel/grayscale.jpg\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/origin/multi-channel/chr30.4.184.jpg\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/origin/multi-channel/BGRA_alpha_60.png\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/origin/multi-channel/BGRA.png\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/origin/license.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/origin/kittens/\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/origin/kittens/not-image.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/origin/kittens/DP802813.jpg\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/origin/kittens/DP153539.jpg\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/origin/kittens/54893.jpg\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/origin/kittens/29.5.a_b_EGDP022204.jpg\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/images/license.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/gmm_data.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/als/\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/als/test.data\n",
            "spark-2.4.5-bin-hadoop2.7/data/mllib/als/sample_movielens_ratings.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/graphx/\n",
            "spark-2.4.5-bin-hadoop2.7/data/graphx/users.txt\n",
            "spark-2.4.5-bin-hadoop2.7/data/graphx/followers.txt\n",
            "spark-2.4.5-bin-hadoop2.7/NOTICE\n",
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/5a/271c416c1c2185b6cb0151b29a91fff6fcaed80173c8584ff6d20e46b465/pyspark-2.4.5.tar.gz (217.8MB)\n",
            "\u001b[K     |████████████████████████████████| 217.8MB 60kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 45.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-2.4.5-py2.py3-none-any.whl size=218257927 sha256=0d074dd7f9d4b1a0e896cedc50f9b0279ccfcbcb31af9198d2f525d64532960a\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/db/04/61d66a5939364e756eb1c1be4ec5bdce6e04047fc7929a3c3c\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEcJToxQZUp4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuiyiWtOZ0A1",
        "colab_type": "text"
      },
      "source": [
        "# Import library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ae15kkZZy7h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import SparkSession, Row, functions as fn\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVc-7zf2aHpx",
        "colab_type": "text"
      },
      "source": [
        "# Create spark session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_t34braZ4Od",
        "colab_type": "code",
        "outputId": "cefa7fbf-fe00-4872-ff50-a7b18de15276",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        }
      },
      "source": [
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://5a373fd13a2a:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v2.4.5</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f006db37f28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpdSSB5UaPpe",
        "colab_type": "text"
      },
      "source": [
        "# Extract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiR0DFsqdNMw",
        "colab_type": "text"
      },
      "source": [
        "ข้อมูลที่ใช้มาจากการทำ ETL ที่ https://colab.research.google.com/drive/145h5YYKzitwuB6uR-LcNFkiIZUGHiK_N?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQpGkK3NaKda",
        "colab_type": "code",
        "outputId": "b1b66d23-fef0-47a7-f418-568d1a64ba2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "covid19_df = spark.read.csv('/content/drive/My Drive/Project/Covid19/single_covid19_data.csv/part-00000-6916f067-9e47-48aa-8d4c-86a4c2571a16-c000.csv', header=True, inferSchema=True)\n",
        "covid19_df.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----+----+-----------+-------------+-----------------+\n",
            "| no| age| sex|nationality|Announce Date|Province of onset|\n",
            "+---+----+----+-----------+-------------+-----------------+\n",
            "|  1|61.0|หญิง|        จีน|     12-01-20|    กรุงเทพมหานคร|\n",
            "|  2|74.0|หญิง|        จีน|     17-01-20|    กรุงเทพมหานคร|\n",
            "|  3|73.0|หญิง|        ไทย|     22-01-20|           นครปฐม|\n",
            "|  4|68.0| ชาย|        จีน|     22-01-20|    กรุงเทพมหานคร|\n",
            "|  5|66.0|หญิง|        จีน|     24-01-20|    กรุงเทพมหานคร|\n",
            "|  6|33.0|หญิง|        จีน|     25-01-20|    กรุงเทพมหานคร|\n",
            "|  7|57.0|หญิง|        จีน|     26-01-20|    กรุงเทพมหานคร|\n",
            "|  8|73.0|หญิง|        จีน|     26-01-20|  ประจวบคีรีขันธ์|\n",
            "|  9|63.0| ชาย|        จีน|     28-01-20|    กรุงเทพมหานคร|\n",
            "| 10|28.0|หญิง|        จีน|     28-01-20|          นนทบุรี|\n",
            "| 11|33.0| ชาย|        จีน|     28-01-20|    กรุงเทพมหานคร|\n",
            "| 12|61.0| ชาย|        จีน|     28-01-20|    กรุงเทพมหานคร|\n",
            "| 13| 6.0| ชาย|        จีน|     28-01-20|    กรุงเทพมหานคร|\n",
            "| 14|32.0|หญิง|        จีน|     28-01-20|           ภูเก็ต|\n",
            "| 15|56.0| ชาย|        จีน|     31-01-20|    กรุงเทพมหานคร|\n",
            "| 16|50.0| ชาย|        ไทย|     31-01-20|    กรุงเทพมหานคร|\n",
            "| 17|28.0| ชาย|        จีน|     31-01-20|        เชียงใหม่|\n",
            "| 18|30.0| ชาย|        จีน|     31-01-20|    กรุงเทพมหานคร|\n",
            "| 19|33.0| ชาย|        จีน|     31-01-20|    กรุงเทพมหานคร|\n",
            "| 20|43.0|หญิง|        ไทย|     04-02-20|    กรุงเทพมหานคร|\n",
            "+---+----+----+-----------+-------------+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ihLuqBHawLb",
        "colab_type": "code",
        "outputId": "9dae587b-efc3-4e05-a18e-f5ad1fc5652c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "covid19_df.count()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2947"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jeRU9XEoAef",
        "colab_type": "text"
      },
      "source": [
        "#Transform\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV5UbdbIefTB",
        "colab_type": "text"
      },
      "source": [
        "นับจำนวนผู้ป่วยต่อวัน"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhUUv05-azps",
        "colab_type": "code",
        "outputId": "e704b0b8-1948-4690-e709-b72725e51347",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "count_covid19_df = covid19_df.groupBy('Announce Date').count()\n",
        "count_covid19_df.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------+-----+\n",
            "|Announce Date|count|\n",
            "+-------------+-----+\n",
            "|     17-04-20|   28|\n",
            "|     05-04-20|  102|\n",
            "|     27-04-20|    9|\n",
            "|     28-01-20|    6|\n",
            "|     03-04-20|  103|\n",
            "|     28-02-20|    1|\n",
            "|     27-03-20|   91|\n",
            "|     12-04-20|   33|\n",
            "|     24-01-20|    1|\n",
            "|     07-03-20|    2|\n",
            "|     11-03-20|    6|\n",
            "|     12-01-20|    1|\n",
            "|     11-02-20|    1|\n",
            "|     20-03-20|   50|\n",
            "|     26-04-20|   15|\n",
            "|     06-04-20|   51|\n",
            "|     18-03-20|   35|\n",
            "|     06-03-20|    1|\n",
            "|     15-04-20|   30|\n",
            "|     13-03-20|    3|\n",
            "+-------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B49mmR-Heixn",
        "colab_type": "text"
      },
      "source": [
        "เปลี่ยน Format วันที่ เพื่อใช้เรียงลำดับตามวัน"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aZXDhAwsFG3",
        "colab_type": "code",
        "outputId": "9848fce1-81ff-4fec-bdec-401134bee7c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "newDate_count_covid19_df = count_covid19_df.withColumn('date', fn.unix_timestamp('Announce Date', 'dd-MM-yy').cast('timestamp'))\n",
        "newDate_count_covid19_df.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------+-----+-------------------+\n",
            "|Announce Date|count|               date|\n",
            "+-------------+-----+-------------------+\n",
            "|     17-04-20|   28|2020-04-17 00:00:00|\n",
            "|     05-04-20|  102|2020-04-05 00:00:00|\n",
            "|     27-04-20|    9|2020-04-27 00:00:00|\n",
            "|     28-01-20|    6|2020-01-28 00:00:00|\n",
            "|     03-04-20|  103|2020-04-03 00:00:00|\n",
            "|     28-02-20|    1|2020-02-28 00:00:00|\n",
            "|     27-03-20|   91|2020-03-27 00:00:00|\n",
            "|     12-04-20|   33|2020-04-12 00:00:00|\n",
            "|     24-01-20|    1|2020-01-24 00:00:00|\n",
            "|     07-03-20|    2|2020-03-07 00:00:00|\n",
            "|     11-03-20|    6|2020-03-11 00:00:00|\n",
            "|     12-01-20|    1|2020-01-12 00:00:00|\n",
            "|     11-02-20|    1|2020-02-11 00:00:00|\n",
            "|     20-03-20|   50|2020-03-20 00:00:00|\n",
            "|     26-04-20|   15|2020-04-26 00:00:00|\n",
            "|     06-04-20|   51|2020-04-06 00:00:00|\n",
            "|     18-03-20|   35|2020-03-18 00:00:00|\n",
            "|     06-03-20|    1|2020-03-06 00:00:00|\n",
            "|     15-04-20|   30|2020-04-15 00:00:00|\n",
            "|     13-03-20|    3|2020-03-13 00:00:00|\n",
            "+-------------+-----+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGBrRweptseG",
        "colab_type": "code",
        "outputId": "397e0628-ccda-4194-e988-41ca454f63bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "ordered_df = newDate_count_covid19_df.select('date', 'count').orderBy('date')\n",
        "ordered_df.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------+-----+\n",
            "|               date|count|\n",
            "+-------------------+-----+\n",
            "|2020-01-12 00:00:00|    1|\n",
            "|2020-01-17 00:00:00|    1|\n",
            "|2020-01-22 00:00:00|    2|\n",
            "|2020-01-24 00:00:00|    1|\n",
            "|2020-01-25 00:00:00|    1|\n",
            "|2020-01-26 00:00:00|    2|\n",
            "|2020-01-28 00:00:00|    6|\n",
            "|2020-01-31 00:00:00|    5|\n",
            "|2020-02-04 00:00:00|    6|\n",
            "|2020-02-08 00:00:00|    7|\n",
            "|2020-02-11 00:00:00|    1|\n",
            "|2020-02-15 00:00:00|    1|\n",
            "|2020-02-17 00:00:00|    1|\n",
            "|2020-02-25 00:00:00|    2|\n",
            "|2020-02-26 00:00:00|    3|\n",
            "|2020-02-28 00:00:00|    1|\n",
            "|2020-02-29 00:00:00|    1|\n",
            "|2020-03-02 00:00:00|    1|\n",
            "|2020-03-05 00:00:00|    4|\n",
            "|2020-03-06 00:00:00|    1|\n",
            "+-------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75qG6QZBezcb",
        "colab_type": "text"
      },
      "source": [
        "จะเห็นว่าวันที่ไม่ต่อเนื่องกัน จึงได้เพิ่มวันที่ที่หายไป โดยใช้ Pandas  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2trtUiyFhjJ",
        "colab_type": "code",
        "outputId": "af885352-1de4-43d8-ac00-2db24094ae1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ordered_df.collect()[-1][0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datetime.datetime(2020, 4, 29, 0, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhfX-vVTEkmO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_date = ordered_df.collect()[0][0]\n",
        "end_date = ordered_df.collect()[-1][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VL25L7SGLd3r",
        "colab_type": "code",
        "outputId": "fe215122-1be9-42bc-d570-45ae19390ecb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "start_date, end_date"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(datetime.datetime(2020, 1, 12, 0, 0), datetime.datetime(2020, 4, 29, 0, 0))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qApqmanxPuaD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "date_series = pd.date_range(start_date, end_date)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wu3kBkqbQfaf",
        "colab_type": "code",
        "outputId": "98086334-55ce-44cc-c0a1-9738d0033b11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "date_series"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatetimeIndex(['2020-01-12', '2020-01-13', '2020-01-14', '2020-01-15',\n",
              "               '2020-01-16', '2020-01-17', '2020-01-18', '2020-01-19',\n",
              "               '2020-01-20', '2020-01-21',\n",
              "               ...\n",
              "               '2020-04-20', '2020-04-21', '2020-04-22', '2020-04-23',\n",
              "               '2020-04-24', '2020-04-25', '2020-04-26', '2020-04-27',\n",
              "               '2020-04-28', '2020-04-29'],\n",
              "              dtype='datetime64[ns]', length=109, freq='D')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 256
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVDWzPNgR1VV",
        "colab_type": "code",
        "outputId": "bf482657-5bc5-4dae-b355-2473282f26f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "pd_date_df = pd.DataFrame({'date': date_series})\n",
        "pd_date_df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-01-12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-01-13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-01-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-01-15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-01-16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>2020-04-25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>2020-04-26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>2020-04-27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>2020-04-28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>2020-04-29</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>109 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          date\n",
              "0   2020-01-12\n",
              "1   2020-01-13\n",
              "2   2020-01-14\n",
              "3   2020-01-15\n",
              "4   2020-01-16\n",
              "..         ...\n",
              "104 2020-04-25\n",
              "105 2020-04-26\n",
              "106 2020-04-27\n",
              "107 2020-04-28\n",
              "108 2020-04-29\n",
              "\n",
              "[109 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg_OhP6CffoX",
        "colab_type": "text"
      },
      "source": [
        "ทำให้ Dataframe ของ Pandas เป็น Dataframe ของ Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2O88m0cFQiyf",
        "colab_type": "code",
        "outputId": "70773b93-018d-489e-8681-a28552a4f8c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "spark_date_df = spark.createDataFrame(pd_date_df)\n",
        "spark_date_df.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------+\n",
            "|               date|\n",
            "+-------------------+\n",
            "|2020-01-12 00:00:00|\n",
            "|2020-01-13 00:00:00|\n",
            "|2020-01-14 00:00:00|\n",
            "|2020-01-15 00:00:00|\n",
            "|2020-01-16 00:00:00|\n",
            "|2020-01-17 00:00:00|\n",
            "|2020-01-18 00:00:00|\n",
            "|2020-01-19 00:00:00|\n",
            "|2020-01-20 00:00:00|\n",
            "|2020-01-21 00:00:00|\n",
            "|2020-01-22 00:00:00|\n",
            "|2020-01-23 00:00:00|\n",
            "|2020-01-24 00:00:00|\n",
            "|2020-01-25 00:00:00|\n",
            "|2020-01-26 00:00:00|\n",
            "|2020-01-27 00:00:00|\n",
            "|2020-01-28 00:00:00|\n",
            "|2020-01-29 00:00:00|\n",
            "|2020-01-30 00:00:00|\n",
            "|2020-01-31 00:00:00|\n",
            "+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGwwqLWoTfnj",
        "colab_type": "code",
        "outputId": "6f69c0c8-68e3-4154-bce0-4e1dccb5e134",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "spark_date_df.count()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "109"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50RebyYEfdlM",
        "colab_type": "text"
      },
      "source": [
        "นำ Dataframe ทั้ง 2 มารวมกันเเล้วให้วันที่เพิ่มมามี count เป็น 0  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8BzvPswT488",
        "colab_type": "code",
        "outputId": "d1da5096-61d4-49f7-933f-ad538ec8d0a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "add_time_df = ordered_df.join(spark_date_df, on='date', how='full').fillna(0).orderBy('date')\n",
        "add_time_df.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------+-----+\n",
            "|               date|count|\n",
            "+-------------------+-----+\n",
            "|2020-01-12 00:00:00|    1|\n",
            "|2020-01-13 00:00:00|    0|\n",
            "|2020-01-14 00:00:00|    0|\n",
            "|2020-01-15 00:00:00|    0|\n",
            "|2020-01-16 00:00:00|    0|\n",
            "|2020-01-17 00:00:00|    1|\n",
            "|2020-01-18 00:00:00|    0|\n",
            "|2020-01-19 00:00:00|    0|\n",
            "|2020-01-20 00:00:00|    0|\n",
            "|2020-01-21 00:00:00|    0|\n",
            "|2020-01-22 00:00:00|    2|\n",
            "|2020-01-23 00:00:00|    0|\n",
            "|2020-01-24 00:00:00|    1|\n",
            "|2020-01-25 00:00:00|    1|\n",
            "|2020-01-26 00:00:00|    2|\n",
            "|2020-01-27 00:00:00|    0|\n",
            "|2020-01-28 00:00:00|    6|\n",
            "|2020-01-29 00:00:00|    0|\n",
            "|2020-01-30 00:00:00|    0|\n",
            "|2020-01-31 00:00:00|    5|\n",
            "+-------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdRtV5frgBJn",
        "colab_type": "text"
      },
      "source": [
        "นับจำนวนผู้ป่วยโดยรวมกับวันก่อนหน้า"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGGtke4plaap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w = Window.rowsBetween(\n",
        "    Window.unboundedPreceding,\n",
        "    Window.currentRow\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzhMyOJjgpKb",
        "colab_type": "code",
        "outputId": "0ad0654f-3102-42ac-b667-cddfd7215403",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "sum_covid19_df = add_time_df.withColumn('sum', fn.sum('count').over(w))\n",
        "sum_covid19_df.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------+-----+---+\n",
            "|               date|count|sum|\n",
            "+-------------------+-----+---+\n",
            "|2020-01-12 00:00:00|    1|  1|\n",
            "|2020-01-13 00:00:00|    0|  1|\n",
            "|2020-01-14 00:00:00|    0|  1|\n",
            "|2020-01-15 00:00:00|    0|  1|\n",
            "|2020-01-16 00:00:00|    0|  1|\n",
            "|2020-01-17 00:00:00|    1|  2|\n",
            "|2020-01-18 00:00:00|    0|  2|\n",
            "|2020-01-19 00:00:00|    0|  2|\n",
            "|2020-01-20 00:00:00|    0|  2|\n",
            "|2020-01-21 00:00:00|    0|  2|\n",
            "|2020-01-22 00:00:00|    2|  4|\n",
            "|2020-01-23 00:00:00|    0|  4|\n",
            "|2020-01-24 00:00:00|    1|  5|\n",
            "|2020-01-25 00:00:00|    1|  6|\n",
            "|2020-01-26 00:00:00|    2|  8|\n",
            "|2020-01-27 00:00:00|    0|  8|\n",
            "|2020-01-28 00:00:00|    6| 14|\n",
            "|2020-01-29 00:00:00|    0| 14|\n",
            "|2020-01-30 00:00:00|    0| 14|\n",
            "|2020-01-31 00:00:00|    5| 19|\n",
            "+-------------------+-----+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPc9h9iEg-Oj",
        "colab_type": "text"
      },
      "source": [
        "สร้าง index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDeDkH9vosSZ",
        "colab_type": "code",
        "outputId": "299ff3d4-932e-404d-cdce-7ac3d479e5ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "index_sum_covid19_df  = sum_covid19_df.withColumn('id', fn.monotonically_increasing_id() + 1)\n",
        "index_sum_covid19_df.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------+-----+---+---+\n",
            "|               date|count|sum| id|\n",
            "+-------------------+-----+---+---+\n",
            "|2020-01-12 00:00:00|    1|  1|  1|\n",
            "|2020-01-13 00:00:00|    0|  1|  2|\n",
            "|2020-01-14 00:00:00|    0|  1|  3|\n",
            "|2020-01-15 00:00:00|    0|  1|  4|\n",
            "|2020-01-16 00:00:00|    0|  1|  5|\n",
            "|2020-01-17 00:00:00|    1|  2|  6|\n",
            "|2020-01-18 00:00:00|    0|  2|  7|\n",
            "|2020-01-19 00:00:00|    0|  2|  8|\n",
            "|2020-01-20 00:00:00|    0|  2|  9|\n",
            "|2020-01-21 00:00:00|    0|  2| 10|\n",
            "|2020-01-22 00:00:00|    2|  4| 11|\n",
            "|2020-01-23 00:00:00|    0|  4| 12|\n",
            "|2020-01-24 00:00:00|    1|  5| 13|\n",
            "|2020-01-25 00:00:00|    1|  6| 14|\n",
            "|2020-01-26 00:00:00|    2|  8| 15|\n",
            "|2020-01-27 00:00:00|    0|  8| 16|\n",
            "|2020-01-28 00:00:00|    6| 14| 17|\n",
            "|2020-01-29 00:00:00|    0| 14| 18|\n",
            "|2020-01-30 00:00:00|    0| 14| 19|\n",
            "|2020-01-31 00:00:00|    5| 19| 20|\n",
            "+-------------------+-----+---+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vL_BFQ_tpag7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_df = index_sum_covid19_df.select('id', 'date', 'count', 'sum')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4fx4oQYuY2Q",
        "colab_type": "text"
      },
      "source": [
        "#Final"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MC_Vd0MCpx14",
        "colab_type": "code",
        "outputId": "e7dc310d-13b9-4996-c25b-3e02596ba103",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "final_df.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-------------------+-----+---+\n",
            "| id|               date|count|sum|\n",
            "+---+-------------------+-----+---+\n",
            "|  1|2020-01-12 00:00:00|    1|  1|\n",
            "|  2|2020-01-13 00:00:00|    0|  1|\n",
            "|  3|2020-01-14 00:00:00|    0|  1|\n",
            "|  4|2020-01-15 00:00:00|    0|  1|\n",
            "|  5|2020-01-16 00:00:00|    0|  1|\n",
            "|  6|2020-01-17 00:00:00|    1|  2|\n",
            "|  7|2020-01-18 00:00:00|    0|  2|\n",
            "|  8|2020-01-19 00:00:00|    0|  2|\n",
            "|  9|2020-01-20 00:00:00|    0|  2|\n",
            "| 10|2020-01-21 00:00:00|    0|  2|\n",
            "| 11|2020-01-22 00:00:00|    2|  4|\n",
            "| 12|2020-01-23 00:00:00|    0|  4|\n",
            "| 13|2020-01-24 00:00:00|    1|  5|\n",
            "| 14|2020-01-25 00:00:00|    1|  6|\n",
            "| 15|2020-01-26 00:00:00|    2|  8|\n",
            "| 16|2020-01-27 00:00:00|    0|  8|\n",
            "| 17|2020-01-28 00:00:00|    6| 14|\n",
            "| 18|2020-01-29 00:00:00|    0| 14|\n",
            "| 19|2020-01-30 00:00:00|    0| 14|\n",
            "| 20|2020-01-31 00:00:00|    5| 19|\n",
            "+---+-------------------+-----+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I1qVrwYpzLh",
        "colab_type": "code",
        "outputId": "1fc83c3a-e404-47b7-942a-f400d31e6857",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "final_df.orderBy(fn.col('Id').desc()).show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-------------------+-----+----+\n",
            "| id|               date|count| sum|\n",
            "+---+-------------------+-----+----+\n",
            "|109|2020-04-29 00:00:00|    9|2947|\n",
            "|108|2020-04-28 00:00:00|    7|2938|\n",
            "|107|2020-04-27 00:00:00|    9|2931|\n",
            "|106|2020-04-26 00:00:00|   15|2922|\n",
            "|105|2020-04-25 00:00:00|   53|2907|\n",
            "|104|2020-04-24 00:00:00|   15|2854|\n",
            "|103|2020-04-23 00:00:00|   13|2839|\n",
            "|102|2020-04-22 00:00:00|   15|2826|\n",
            "|101|2020-04-21 00:00:00|   19|2811|\n",
            "|100|2020-04-20 00:00:00|   27|2792|\n",
            "| 99|2020-04-19 00:00:00|   32|2765|\n",
            "| 98|2020-04-18 00:00:00|   33|2733|\n",
            "| 97|2020-04-17 00:00:00|   28|2700|\n",
            "| 96|2020-04-16 00:00:00|   29|2672|\n",
            "| 95|2020-04-15 00:00:00|   30|2643|\n",
            "| 94|2020-04-14 00:00:00|   34|2613|\n",
            "| 93|2020-04-13 00:00:00|   28|2579|\n",
            "| 92|2020-04-12 00:00:00|   33|2551|\n",
            "| 91|2020-04-11 00:00:00|   45|2518|\n",
            "| 90|2020-04-10 00:00:00|   50|2473|\n",
            "+---+-------------------+-----+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlBJ981LuipJ",
        "colab_type": "text"
      },
      "source": [
        "#Load to BigQuery\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KK8DteJcp_T7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_df.coalesce(1).write.csv('./THCovid19_sum.csv', header=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaQZJzFOONRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! gcloud auth login"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQNwvomAOXbl",
        "colab_type": "code",
        "outputId": "e540b437-78fd-4e2b-aa4f-bca7560ec370",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "! gcloud config set project cobalt-bond-272904"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARVsS7F6OiFD",
        "colab_type": "code",
        "outputId": "2f03bef2-b9d9-4e66-9ea7-459f910513a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "! gsutil cp ./THCovid19_sum.csv/part-00000-c98aae8c-a3ed-477e-ae94-8ee343ca66ed-c000.csv gs://thanavi_project_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://./THCovid19_sum.csv/part-00000-c98aae8c-a3ed-477e-ae94-8ee343ca66ed-c000.csv [Content-Type=text/csv]...\n",
            "/ [1 files][  3.7 KiB/  3.7 KiB]                                                \n",
            "Operation completed over 1 objects/3.7 KiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jlunb-P4OxGY",
        "colab_type": "code",
        "outputId": "e520b178-c842-4b55-db33-3982d62e2fed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "! gsutil mv gs://thanavi_project_data/part-00000-c98aae8c-a3ed-477e-ae94-8ee343ca66ed-c000.csv gs://thanavi_project_data/THCovid19_sum.csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://thanavi_project_data/part-00000-c98aae8c-a3ed-477e-ae94-8ee343ca66ed-c000.csv [Content-Type=text/csv]...\n",
            "/ [0 files][    0.0 B/  3.7 KiB]                                                \r/ [1 files][  3.7 KiB/  3.7 KiB]                                                \rRemoving gs://thanavi_project_data/part-00000-c98aae8c-a3ed-477e-ae94-8ee343ca66ed-c000.csv...\n",
            "\n",
            "Operation completed over 1 objects/3.7 KiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVj01LylPAyv",
        "colab_type": "code",
        "outputId": "1a2098b9-42fb-41f3-c9b0-9b49b179d4e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "! gsutil ls gs://thanavi_project_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gs://thanavi_project_data/THCovid19_Data.csv\n",
            "gs://thanavi_project_data/THCovid19_sum.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJmk70CwPD9i",
        "colab_type": "code",
        "outputId": "ad029f7d-f233-4cb5-89f0-9561ae78152f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "! bq ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Welcome to BigQuery! This script will walk you through the \n",
            "process of initializing your .bigqueryrc configuration file.\n",
            "\n",
            "First, we need to set up your credentials if they do not \n",
            "already exist.\n",
            "\n",
            "Credential creation complete. Now we will select a default project.\n",
            "\n",
            "List of projects:\n",
            "  #       projectId          friendlyName    \n",
            " --- -------------------- ------------------ \n",
            "  1   cobalt-bond-272904   My First Project  \n",
            "  2   r2de-275802          R2DE              \n",
            "Found multiple projects. Please enter a selection for \n",
            "which should be the default, or leave blank to not \n",
            "set a default.\n",
            "\n",
            "Enter a selection (1 - 2): 1\n",
            "\n",
            "BigQuery configuration complete! Type \"bq\" to get started.\n",
            "\n",
            "     datasetId     \n",
            " ----------------- \n",
            "  Starbuck_Stores  \n",
            "  covid19          \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfYLvmirPJdP",
        "colab_type": "code",
        "outputId": "94bf29bf-02ef-4a98-c959-6115ec850f5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "! bq load --autodetect covid19.THCovid19_Sum gs://thanavi_project_data/THCovid19_sum.csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Waiting on bqjob_r795e5a5f8e4512d1_00000171da44777f_1 ... (3s) Current status: DONE   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTR3vo73PQRZ",
        "colab_type": "code",
        "outputId": "97f13eb4-9618-49b0-d91d-c5681bccb889",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "! bq head -n 10 covid19.THCovid19_Sum"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----+---------------------+-------+-----+\n",
            "| id |        date         | count | sum |\n",
            "+----+---------------------+-------+-----+\n",
            "|  2 | 2020-01-13 00:00:00 |     0 |   1 |\n",
            "|  3 | 2020-01-14 00:00:00 |     0 |   1 |\n",
            "|  4 | 2020-01-15 00:00:00 |     0 |   1 |\n",
            "|  5 | 2020-01-16 00:00:00 |     0 |   1 |\n",
            "|  7 | 2020-01-18 00:00:00 |     0 |   2 |\n",
            "|  8 | 2020-01-19 00:00:00 |     0 |   2 |\n",
            "|  9 | 2020-01-20 00:00:00 |     0 |   2 |\n",
            "| 10 | 2020-01-21 00:00:00 |     0 |   2 |\n",
            "| 12 | 2020-01-23 00:00:00 |     0 |   4 |\n",
            "| 16 | 2020-01-27 00:00:00 |     0 |   8 |\n",
            "+----+---------------------+-------+-----+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajI-0pr6PVX3",
        "colab_type": "code",
        "outputId": "4b30bf46-52eb-4d8e-82a0-e8a1d6d29bda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 823
        }
      },
      "source": [
        "! bq help head"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python script for interacting with BigQuery.\n",
            "\n",
            "\n",
            "USAGE: bq.py [--global_flags] <command> [--command_flags] [args]\n",
            "\n",
            "\n",
            "head            Displays rows in a table.\n",
            "\n",
            "                Examples:\n",
            "                bq head dataset.table\n",
            "                bq head -j job\n",
            "                bq head -n 10 dataset.table\n",
            "                bq head -s 5 -n 10 dataset.table\n",
            "\n",
            "                Flags for head:\n",
            "\n",
            "                  /tools/google-cloud-sdk/platform/bq/bq.py:\n",
            "                    -j,--[no]job: Reads the results of a query job.\n",
            "                      (default: 'false')\n",
            "                    -n,--max_rows: The number of rows to print when showing\n",
            "                      table data.\n",
            "                      (default: '100')\n",
            "                      (an integer)\n",
            "                    -c,--selected_fields: A subset of fields (including nested\n",
            "                      fields) to return when showing table data. If not\n",
            "                      specified, full row will be retrieved. For example,\n",
            "                      \"-c:a,b\".\n",
            "                    -s,--start_row: The number of rows to skip before showing\n",
            "                      table data.\n",
            "                      (default: '0')\n",
            "                      (an integer)\n",
            "                    -t,--[no]table: Reads rows from a table.\n",
            "                      (default: 'false')\n",
            "\n",
            "                  absl.flags:\n",
            "                    --flagfile: Insert flag definitions from the given file into\n",
            "                      the command line.\n",
            "                      (default: '')\n",
            "                    --undefok: comma-separated list of flag names that it is\n",
            "                      okay to specify on the command line even if the program\n",
            "                      does not define a flag with that name.  IMPORTANT: flags\n",
            "                      in this list that have arguments MUST use the --flag=value\n",
            "                      format.\n",
            "                      (default: '')\n",
            "\n",
            "\n",
            "Run 'bq.py --help' to get help for global flags.\n",
            "Run 'bq.py help' to see the list of available commands.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKFFyYx7R2G8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}